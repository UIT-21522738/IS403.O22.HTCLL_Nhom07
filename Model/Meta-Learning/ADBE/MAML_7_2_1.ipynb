{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import higher\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "# import torchtext\n",
    "import os\n",
    "import torch.optim as optim\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2019</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>224.570007</td>\n",
       "      <td>224.570007</td>\n",
       "      <td>226.169998</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>219.910004</td>\n",
       "      <td>2784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2019</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>215.699997</td>\n",
       "      <td>215.699997</td>\n",
       "      <td>223.630005</td>\n",
       "      <td>215.149994</td>\n",
       "      <td>220.880005</td>\n",
       "      <td>3663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/4/2019</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>226.190002</td>\n",
       "      <td>226.190002</td>\n",
       "      <td>227.649994</td>\n",
       "      <td>217.479996</td>\n",
       "      <td>219.839996</td>\n",
       "      <td>4043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2019</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>229.259995</td>\n",
       "      <td>229.259995</td>\n",
       "      <td>232.600006</td>\n",
       "      <td>227.289993</td>\n",
       "      <td>229.949997</td>\n",
       "      <td>3638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2019</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>232.679993</td>\n",
       "      <td>232.679993</td>\n",
       "      <td>233.770004</td>\n",
       "      <td>228.330002</td>\n",
       "      <td>232.649994</td>\n",
       "      <td>3685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>3/19/2024</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>521.190002</td>\n",
       "      <td>521.190002</td>\n",
       "      <td>522.679993</td>\n",
       "      <td>508.989990</td>\n",
       "      <td>509.890015</td>\n",
       "      <td>7081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>3/20/2024</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>519.140015</td>\n",
       "      <td>519.140015</td>\n",
       "      <td>523.869995</td>\n",
       "      <td>512.500000</td>\n",
       "      <td>523.289978</td>\n",
       "      <td>4239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>3/21/2024</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>511.250000</td>\n",
       "      <td>511.250000</td>\n",
       "      <td>519.729981</td>\n",
       "      <td>506.200012</td>\n",
       "      <td>517.599976</td>\n",
       "      <td>5206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>3/22/2024</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>499.519989</td>\n",
       "      <td>499.519989</td>\n",
       "      <td>511.589996</td>\n",
       "      <td>496.670013</td>\n",
       "      <td>509.070007</td>\n",
       "      <td>5410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>3/25/2024</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>507.230011</td>\n",
       "      <td>507.230011</td>\n",
       "      <td>510.570007</td>\n",
       "      <td>496.690002</td>\n",
       "      <td>496.700012</td>\n",
       "      <td>4265618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1316 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Symbol   Adj Close       Close        High         Low  \\\n",
       "0      1/2/2019   ADBE  224.570007  224.570007  226.169998  219.000000   \n",
       "1      1/3/2019   ADBE  215.699997  215.699997  223.630005  215.149994   \n",
       "2      1/4/2019   ADBE  226.190002  226.190002  227.649994  217.479996   \n",
       "3      1/7/2019   ADBE  229.259995  229.259995  232.600006  227.289993   \n",
       "4      1/8/2019   ADBE  232.679993  232.679993  233.770004  228.330002   \n",
       "...         ...    ...         ...         ...         ...         ...   \n",
       "1311  3/19/2024   ADBE  521.190002  521.190002  522.679993  508.989990   \n",
       "1312  3/20/2024   ADBE  519.140015  519.140015  523.869995  512.500000   \n",
       "1313  3/21/2024   ADBE  511.250000  511.250000  519.729981  506.200012   \n",
       "1314  3/22/2024   ADBE  499.519989  499.519989  511.589996  496.670013   \n",
       "1315  3/25/2024   ADBE  507.230011  507.230011  510.570007  496.690002   \n",
       "\n",
       "            Open   Volume  \n",
       "0     219.910004  2784100  \n",
       "1     220.880005  3663500  \n",
       "2     219.839996  4043400  \n",
       "3     229.949997  3638500  \n",
       "4     232.649994  3685900  \n",
       "...          ...      ...  \n",
       "1311  509.890015  7081800  \n",
       "1312  523.289978  4239600  \n",
       "1313  517.599976  5206600  \n",
       "1314  509.070007  5410300  \n",
       "1315  496.700012  4265618  \n",
       "\n",
       "[1316 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../Dataset/ADBE_Stock.csv\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       224.570007\n",
       "1       215.699997\n",
       "2       226.190002\n",
       "3       229.259995\n",
       "4       232.679993\n",
       "           ...    \n",
       "1311    521.190002\n",
       "1312    519.140015\n",
       "1313    511.250000\n",
       "1314    499.519989\n",
       "1315    507.230011\n",
       "Name: Close, Length: 1316, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.reset_index()['Close']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01876576],\n",
       "       [0.        ],\n",
       "       [0.02219309],\n",
       "       ...,\n",
       "       [0.62527769],\n",
       "       [0.6004612 ],\n",
       "       [0.61677283]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "df1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chia train test\n",
    "train_size = int(0.7 * len(df1))\n",
    "test_size = int(0.2 * len(df1))\n",
    "val_size = len(df1) - train_size - test_size\n",
    "\n",
    "train_data = df1[:train_size]\n",
    "test_data = df1[train_size:train_size+test_size]\n",
    "val_data = df1[train_size+test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-time_step-1):\n",
    "\t\ta = dataset[i:(i+time_step), 0]   ###i=0, X=0,1,2,3-----99   Y=100 \n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + time_step, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 100\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_val, yval = create_dataset(val_data, time_step)\n",
    "X_test, ytest = create_dataset(test_data, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Reshape input to be [samples, time steps, features] which is required for \n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "X_val = X_val.reshape(X_val.shape[0],X_val.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\2200389102.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\2200389102.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\2200389102.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(ytest, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\935544513.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\935544513.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\935544513.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\935544513.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "class StockPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        out = self.fc(h[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 1  # Số lượng đặc trưng đầu vào\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "inner_steps = 5\n",
    "\n",
    "model = StockPredictor(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Chuyển đổi dữ liệu thành tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\734704579.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train_list = [torch.tensor(x, dtype=torch.float32) for x in X_train]\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\734704579.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_list = [torch.tensor(y, dtype=torch.float32) for y in y_train]\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\734704579.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_list = [torch.tensor(x, dtype=torch.float32) for x in X_test]\n",
      "C:\\Users\\tuleh\\AppData\\Local\\Temp\\ipykernel_8588\\734704579.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_list = [torch.tensor(y, dtype=torch.float32) for y in y_test]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m x_test_list \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_test]\n\u001b[0;32m     49\u001b[0m y_test_list \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_test]\n\u001b[1;32m---> 51\u001b[0m \u001b[43mmaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[87], line 36\u001b[0m, in \u001b[0;36mMAML.train\u001b[1;34m(self, x_train_list, y_train_list, x_test_list, y_test_list)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_train_list, y_train_list, x_test_list, y_test_list):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_train, y_train, x_test, y_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_train_list, y_train_list, x_test_list, y_test_list):\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[87], line 23\u001b[0m, in \u001b[0;36mMAML.meta_update\u001b[1;34m(self, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_train, y_train, x_test, y_test):\n\u001b[1;32m---> 23\u001b[0m     adapted_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Outer loop\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     test_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_test)\n",
      "Cell \u001b[1;32mIn[87], line 15\u001b[0m, in \u001b[0;36mMAML.inner_update\u001b[1;34m(self, x_train, y_train)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Inner loop\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_steps):\n\u001b[1;32m---> 15\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(predictions, y_train)\n\u001b[0;32m     17\u001b[0m     grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[86], line 9\u001b[0m, in \u001b[0;36mStockPredictor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      8\u001b[0m     h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "class MAML:\n",
    "    def __init__(self, model, inner_lr, outer_lr, inner_steps):\n",
    "        self.model = model\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=outer_lr)\n",
    "    \n",
    "    def inner_update(self, x_train, y_train):\n",
    "        # Clone model parameters\n",
    "        adapted_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "        \n",
    "        # Inner loop\n",
    "        for _ in range(self.inner_steps):\n",
    "            predictions = self.model(x_train)\n",
    "            loss = nn.MSELoss()(predictions, y_train)\n",
    "            grads = torch.autograd.grad(loss, self.model.parameters(), create_graph=True)\n",
    "            adapted_params = {name: param - self.inner_lr * grad for (name, param), grad in zip(adapted_params.items(), grads)}\n",
    "        \n",
    "        return adapted_params\n",
    "    \n",
    "    def meta_update(self, x_train, y_train, x_test, y_test):\n",
    "        adapted_params = self.inner_update(x_train, y_train)\n",
    "        \n",
    "        # Outer loop\n",
    "        test_predictions = self.model(x_test)\n",
    "        test_loss = nn.MSELoss()(test_predictions, y_test)\n",
    "        \n",
    "        # Meta optimization\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        test_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "\n",
    "    def train(self, x_train_list, y_train_list, x_test_list, y_test_list):\n",
    "        for x_train, y_train, x_test, y_test in zip(x_train_list, y_train_list, x_test_list, y_test_list):\n",
    "            self.meta_update(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Training loop\n",
    "input_size = 1   # Số lượng features\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "model = StockPredictor(input_size, hidden_size, output_size)\n",
    "maml = MAML(model, inner_lr=0.01, outer_lr=0.001, inner_steps=1)\n",
    "\n",
    "# Chuyển đổi thành danh sách tensors để sử dụng trong MAML\n",
    "x_train_list = [torch.tensor(x, dtype=torch.float32) for x in X_train]\n",
    "y_train_list = [torch.tensor(y, dtype=torch.float32) for y in y_train]\n",
    "x_test_list = [torch.tensor(x, dtype=torch.float32) for x in X_test]\n",
    "y_test_list = [torch.tensor(y, dtype=torch.float32) for y in y_test]\n",
    "\n",
    "maml.train(x_train_list, y_train_list, x_test_list, y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Meta Loss: 154369.63037109375\n",
      "Epoch 2, Meta Loss: 200709.34521484375\n",
      "Epoch 3, Meta Loss: 169081.9453125\n",
      "Epoch 4, Meta Loss: 167268.32299804688\n",
      "Epoch 5, Meta Loss: 171969.3359375\n",
      "Epoch 6, Meta Loss: 166254.6005859375\n",
      "Epoch 7, Meta Loss: 155000.42138671875\n",
      "Epoch 8, Meta Loss: 253423.7470703125\n",
      "Epoch 9, Meta Loss: 171204.455078125\n",
      "Epoch 10, Meta Loss: 149157.10473632812\n",
      "Epoch 11, Meta Loss: 187365.56396484375\n",
      "Epoch 12, Meta Loss: 188601.49658203125\n",
      "Epoch 13, Meta Loss: 203994.4296875\n",
      "Epoch 14, Meta Loss: 177572.41625976562\n",
      "Epoch 15, Meta Loss: 218623.72827148438\n",
      "Epoch 16, Meta Loss: 179691.41162109375\n",
      "Epoch 17, Meta Loss: 193367.2822265625\n",
      "Epoch 18, Meta Loss: 200440.75830078125\n",
      "Epoch 19, Meta Loss: 171624.32250976562\n",
      "Epoch 20, Meta Loss: 172451.3154296875\n",
      "Epoch 21, Meta Loss: 224770.54736328125\n",
      "Epoch 22, Meta Loss: 162193.18920898438\n",
      "Epoch 23, Meta Loss: 197079.46923828125\n",
      "Epoch 24, Meta Loss: 225348.51708984375\n",
      "Epoch 25, Meta Loss: 133157.04467773438\n",
      "Epoch 26, Meta Loss: 171928.68017578125\n",
      "Epoch 27, Meta Loss: 166025.95458984375\n",
      "Epoch 28, Meta Loss: 207076.30615234375\n",
      "Epoch 29, Meta Loss: 194779.1494140625\n",
      "Epoch 30, Meta Loss: 180785.33642578125\n",
      "Epoch 31, Meta Loss: 195389.07421875\n",
      "Epoch 32, Meta Loss: 144634.9033203125\n",
      "Epoch 33, Meta Loss: 229486.47216796875\n",
      "Epoch 34, Meta Loss: 160203.17333984375\n",
      "Epoch 35, Meta Loss: 245070.6533203125\n",
      "Epoch 36, Meta Loss: 186780.765625\n",
      "Epoch 37, Meta Loss: 160293.11767578125\n",
      "Epoch 38, Meta Loss: 202455.7060546875\n",
      "Epoch 39, Meta Loss: 166063.58129882812\n",
      "Epoch 40, Meta Loss: 148411.533203125\n",
      "Epoch 41, Meta Loss: 239562.38232421875\n",
      "Epoch 42, Meta Loss: 202482.81201171875\n",
      "Epoch 43, Meta Loss: 198900.27392578125\n",
      "Epoch 44, Meta Loss: 182609.87255859375\n",
      "Epoch 45, Meta Loss: 228196.27197265625\n",
      "Epoch 46, Meta Loss: 196745.12060546875\n",
      "Epoch 47, Meta Loss: 222789.9853515625\n",
      "Epoch 48, Meta Loss: 196777.65991210938\n",
      "Epoch 49, Meta Loss: 204044.0654296875\n",
      "Epoch 50, Meta Loss: 221169.94067382812\n",
      "Epoch 51, Meta Loss: 194380.10668945312\n",
      "Epoch 52, Meta Loss: 185209.29931640625\n",
      "Epoch 53, Meta Loss: 195032.697265625\n",
      "Epoch 54, Meta Loss: 216754.994140625\n",
      "Epoch 55, Meta Loss: 187932.09301757812\n",
      "Epoch 56, Meta Loss: 177717.73559570312\n",
      "Epoch 57, Meta Loss: 145698.57861328125\n",
      "Epoch 58, Meta Loss: 167198.38330078125\n",
      "Epoch 59, Meta Loss: 185008.55078125\n",
      "Epoch 60, Meta Loss: 191963.67333984375\n",
      "Epoch 61, Meta Loss: 180920.68579101562\n",
      "Epoch 62, Meta Loss: 228367.42724609375\n",
      "Epoch 63, Meta Loss: 171906.3251953125\n",
      "Epoch 64, Meta Loss: 170149.84838867188\n",
      "Epoch 65, Meta Loss: 211215.1787109375\n",
      "Epoch 66, Meta Loss: 178902.07006835938\n",
      "Epoch 67, Meta Loss: 140943.9306640625\n",
      "Epoch 68, Meta Loss: 217924.67041015625\n",
      "Epoch 69, Meta Loss: 197368.279296875\n",
      "Epoch 70, Meta Loss: 193206.916015625\n",
      "Epoch 71, Meta Loss: 196887.55029296875\n",
      "Epoch 72, Meta Loss: 182546.30126953125\n",
      "Epoch 73, Meta Loss: 199315.93432617188\n",
      "Epoch 74, Meta Loss: 186323.85473632812\n",
      "Epoch 75, Meta Loss: 162105.66772460938\n",
      "Epoch 76, Meta Loss: 181393.64013671875\n",
      "Epoch 77, Meta Loss: 186843.73681640625\n",
      "Epoch 78, Meta Loss: 195796.12719726562\n",
      "Epoch 79, Meta Loss: 193536.8564453125\n",
      "Epoch 80, Meta Loss: 189258.43701171875\n",
      "Epoch 81, Meta Loss: 217502.2265625\n",
      "Epoch 82, Meta Loss: 173640.06958007812\n",
      "Epoch 83, Meta Loss: 223788.837890625\n",
      "Epoch 84, Meta Loss: 219300.66796875\n",
      "Epoch 85, Meta Loss: 188893.76538085938\n",
      "Epoch 86, Meta Loss: 192476.53759765625\n",
      "Epoch 87, Meta Loss: 224559.29467773438\n",
      "Epoch 88, Meta Loss: 183538.86962890625\n",
      "Epoch 89, Meta Loss: 239117.8994140625\n",
      "Epoch 90, Meta Loss: 207973.54345703125\n",
      "Epoch 91, Meta Loss: 230338.51708984375\n",
      "Epoch 92, Meta Loss: 220442.923828125\n",
      "Epoch 93, Meta Loss: 263762.0654296875\n",
      "Epoch 94, Meta Loss: 263648.69775390625\n",
      "Epoch 95, Meta Loss: 207044.13818359375\n",
      "Epoch 96, Meta Loss: 208683.49462890625\n",
      "Epoch 97, Meta Loss: 194129.3388671875\n",
      "Epoch 98, Meta Loss: 202929.9697265625\n",
      "Epoch 99, Meta Loss: 237340.25439453125\n",
      "Epoch 100, Meta Loss: 195610.06079101562\n",
      "Predictions: [0.11601176112890244, 0.11605051159858704, 0.11612166464328766, 0.11619336158037186, 0.11621978133916855, 0.11622893065214157, 0.11629509925842285, 0.1163041740655899, 0.11628174036741257, 0.11633044481277466, 0.11632297933101654, 0.11635057628154755, 0.11633823812007904, 0.11634849011898041, 0.11635273694992065, 0.11637028306722641, 0.1163722574710846, 0.11638355255126953, 0.1163872554898262, 0.11638696491718292, 0.11639146506786346, 0.11638639867305756, 0.11637449264526367, 0.11637379229068756, 0.11636117100715637, 0.11635306477546692, 0.11633817106485367, 0.1163608655333519, 0.11636888235807419, 0.11638906598091125, 0.11640296876430511, 0.11639919877052307, 0.11632040143013, 0.11630226671695709, 0.11633767187595367, 0.11635510623455048, 0.11634009331464767, 0.11634495109319687, 0.1163439154624939, 0.11634223163127899, 0.11633720993995667, 0.1163349598646164, 0.1163371354341507, 0.11629411578178406, 0.11625955253839493, 0.11623864620923996, 0.11622712761163712, 0.11628121137619019, 0.11630454659461975, 0.11631971597671509, 0.11633700877428055, 0.11633788049221039, 0.11633997410535812, 0.11633697897195816, 0.11633074283599854, 0.11636440455913544, 0.11635889112949371, 0.11634212732315063, 0.11635760962963104, 0.11638471484184265, 0.11638130247592926, 0.11639533191919327, 0.11640103906393051, 0.11638908833265305, 0.11639519035816193, 0.1164049506187439, 0.11640526354312897, 0.11637425422668457, 0.1163768619298935, 0.11638088524341583, 0.11639367043972015, 0.11638037115335464, 0.1163557916879654, 0.11635646224021912, 0.116326704621315, 0.11615795642137527, 0.11611427366733551, 0.11609388142824173, 0.1160871610045433, 0.11616727709770203, 0.11620460450649261, 0.11617261171340942, 0.11616609245538712, 0.11620310693979263, 0.11624772101640701, 0.11624154448509216, 0.11613866686820984, 0.11611969023942947, 0.11618124693632126, 0.11616696417331696, 0.11620365083217621, 0.11627504229545593, 0.11626513302326202, 0.1162518560886383, 0.11577359586954117, 0.11591705679893494, 0.11597628146409988, 0.11596518009901047, 0.11590754240751266, 0.11581091582775116]\n",
      "Targets: [532.0599975585938, 544.5, 558.7100219726562, 563.6599731445312, 565.4500122070312, 585.2000122070312, 585.3099975585938, 577.739990234375, 597.219970703125, 590.3400268554688, 604.3300170898438, 595.3099975585938, 602.0599975585938, 602.6599731445312, 612.7000122070312, 610.989990234375, 619.719970703125, 619.4299926757812, 619.27001953125, 623.3200073242188, 617.3900146484375, 611.010009765625, 612.469970703125, 604.5599975585938, 602.219970703125, 595.7000122070312, 608.780029296875, 610.010009765625, 625.2000122070312, 633.6599731445312, 624.260009765625, 584.6400146484375, 584.6799926757812, 599.1300048828125, 604.6400146484375, 596.0599975585938, 600.1400146484375, 598.75, 598.260009765625, 596.0800170898438, 595.52001953125, 596.5999755859375, 580.0700073242188, 571.7899780273438, 567.0499877929688, 564.5999755859375, 580.5499877929688, 586.2000122070312, 591.030029296875, 597.489990234375, 596.5399780273438, 597.6799926757812, 596.0999755859375, 593.7000122070312, 611.5499877929688, 603.5900268554688, 597.1799926757812, 606.47998046875, 622.5800170898438, 613.9299926757812, 630.22998046875, 627.9600219726562, 617.780029296875, 627.9099731445312, 634.760009765625, 630.5, 607.1400146484375, 615.8499755859375, 615.8599853515625, 627.2100219726562, 611.8400268554688, 601.8900146484375, 604.6599731445312, 590.4400024414062, 546.6599731445312, 541.9099731445312, 538.52001953125, 537.5700073242188, 553.4400024414062, 560.47998046875, 552.489990234375, 551.8200073242188, 560.280029296875, 570.9299926757812, 567.9400024414062, 544.8400268554688, 543.0900268554688, 556.0399780273438, 551.6900024414062, 560.4199829101562, 579.1400146484375, 573.5499877929688, 570.4500122070312, 492.4599914550781, 513.8599853515625, 521.1900024414062, 519.1400146484375, 511.25, 499.5199890136719, 507.2300109863281]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 1. Tiền xử lý dữ liệu\n",
    "def create_time_series(data, window_size):\n",
    "    series = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        series.append(data[i:i + window_size + 1])\n",
    "    return np.array(series)\n",
    "\n",
    "# 2. Xây dựng mô hình MAML\n",
    "class StockPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        h = h[:, -1, :]\n",
    "        return self.fc(h)\n",
    "\n",
    "def train_maml(model, data, epochs, meta_lr, task_lr, meta_batch_size):\n",
    "    optimizer = Adam(model.parameters(), lr=meta_lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        meta_loss = 0\n",
    "        for _ in range(meta_batch_size):\n",
    "            task_data = data[np.random.choice(len(data), meta_batch_size)]\n",
    "            task_model = StockPredictor(input_size, hidden_size).to(device)\n",
    "            task_model.load_state_dict(model.state_dict())\n",
    "            task_optimizer = Adam(task_model.parameters(), lr=task_lr)\n",
    "\n",
    "            for _ in range(task_epochs):\n",
    "                for sequence in task_data:\n",
    "                    x, y = sequence[:-1], sequence[-1]\n",
    "                    x = torch.tensor(x, dtype=torch.float32).to(device).unsqueeze(-1)  # Thêm chiều cho đầu vào LSTM\n",
    "                    y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "                    y_pred = task_model(x.unsqueeze(0))\n",
    "                    task_loss = loss_fn(y_pred, y.unsqueeze(0))\n",
    "                    task_optimizer.zero_grad()\n",
    "                    task_loss.backward()\n",
    "                    task_optimizer.step()\n",
    "\n",
    "            meta_loss += task_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        meta_loss_tensor = torch.tensor(meta_loss, requires_grad=True)  # Đảm bảo meta_loss là tensor\n",
    "        meta_loss_tensor.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}, Meta Loss: {meta_loss / meta_batch_size}')\n",
    "\n",
    "# 3. Đánh giá mô hình\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequence in data:\n",
    "            x, y = sequence[:-1], sequence[-1]\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(device).unsqueeze(-1)  # Thêm chiều cho đầu vào LSTM\n",
    "            y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "            y_pred = model(x.unsqueeze(0))\n",
    "            predictions.append(y_pred.item())\n",
    "            targets.append(y.item())\n",
    "    return predictions, targets\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Giả sử bạn đã có dữ liệu trong DataFrame\n",
    "    # Hãy thay thế `your_data` bằng DataFrame của bạn\n",
    "    your_data = pd.read_csv('../../../Dataset/ADBE_Stock.csv')\n",
    "    close_prices = your_data['Close'].values\n",
    "\n",
    "    window_size = 20\n",
    "    series = create_time_series(close_prices, window_size)\n",
    "\n",
    "    input_size = 1\n",
    "    hidden_size = 50\n",
    "    task_epochs = 5\n",
    "    meta_lr = 0.001\n",
    "    task_lr = 0.01\n",
    "    meta_batch_size = 16\n",
    "    epochs = 100\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = StockPredictor(input_size, hidden_size).to(device)\n",
    "\n",
    "    train_maml(model, series, epochs, meta_lr, task_lr, meta_batch_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions, targets = evaluate(model, series[-100:])\n",
    "    print(f'Predictions: {predictions}')\n",
    "    print(f'Targets: {targets}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(input_shape)  \u001b[38;5;66;03m# Đảm bảo hình dạng đầu vào đúng\u001b[39;00m\n\u001b[0;32m    108\u001b[0m maml \u001b[38;5;241m=\u001b[39m MAML(model, meta_lr, task_lr)\n\u001b[1;32m--> 109\u001b[0m \u001b[43mmaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Đánh giá mô hình\u001b[39;00m\n\u001b[0;32m    112\u001b[0m test_task \u001b[38;5;241m=\u001b[39m prepared_tasks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Sử dụng một task từ tập dữ liệu đã chuẩn bị để kiểm tra\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[101], line 73\u001b[0m, in \u001b[0;36mMAML.meta_train\u001b[1;34m(self, tasks, num_epochs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     72\u001b[0m     all_gradients\u001b[38;5;241m.\u001b[39mappend(gradients)\n\u001b[1;32m---> 73\u001b[0m mean_gradients \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_gradients\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(mean_gradients, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn[101], line 73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     72\u001b[0m     all_gradients\u001b[38;5;241m.\u001b[39mappend(gradients)\n\u001b[1;32m---> 73\u001b[0m mean_gradients \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m gradients \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_gradients)]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(mean_gradients, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data['Close'].values\n",
    "\n",
    "# Tạo các tasks\n",
    "def create_tasks(data, task_size, num_tasks):\n",
    "    tasks = []\n",
    "    for _ in range(num_tasks):\n",
    "        start_idx = np.random.randint(0, len(data) - task_size)\n",
    "        task_data = data[start_idx:start_idx + task_size]\n",
    "        tasks.append(task_data)\n",
    "    return tasks\n",
    "\n",
    "# Định nghĩa mô hình\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Chuẩn bị dữ liệu cho từng task\n",
    "def prepare_task_data(tasks, look_back=1):\n",
    "    prepared_tasks = []\n",
    "    for task in tasks:\n",
    "        X, y = [], []\n",
    "        for i in range(len(task) - look_back):\n",
    "            X.append(task[i:i + look_back])\n",
    "            y.append(task[i + look_back])\n",
    "        X = np.array(X).reshape(-1, look_back)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        split_index = int(len(X) * 0.8)\n",
    "        X_train, y_train = X[:split_index], y[:split_index]\n",
    "        X_val, y_val = X[split_index:], y[split_index:]\n",
    "        prepared_tasks.append({'X_train': X_train, 'y_train': y_train, 'X_val': X_val, 'y_val': y_val})\n",
    "    return prepared_tasks\n",
    "\n",
    "# Áp dụng MAML\n",
    "class MAML:\n",
    "    def __init__(self, model, meta_lr, task_lr):\n",
    "        self.model = model\n",
    "        self.meta_optimizer = tf.keras.optimizers.Adam(meta_lr)\n",
    "        self.task_lr = task_lr\n",
    "\n",
    "    def adapt(self, task_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(task_data['X_train'], training=True)\n",
    "            loss = tf.keras.losses.MSE(task_data['y_train'], predictions)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        k_shot_model = build_model(task_data['X_train'].shape[1:])\n",
    "        k_shot_model.set_weights(self.model.get_weights())\n",
    "        k_shot_optimizer = tf.keras.optimizers.Adam(self.task_lr)\n",
    "        k_shot_optimizer.apply_gradients(zip(gradients, k_shot_model.trainable_variables))\n",
    "        return k_shot_model\n",
    "\n",
    "    def meta_train(self, tasks, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            all_gradients = []\n",
    "            for task_data in tasks:\n",
    "                k_shot_model = self.adapt(task_data)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predictions = k_shot_model(task_data['X_val'], training=False)\n",
    "                    loss = tf.keras.losses.MSE(task_data['y_val'], predictions)\n",
    "                gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "                all_gradients.append(gradients)\n",
    "            mean_gradients = [tf.reduce_mean(gradients, axis=0) for gradients in zip(*all_gradients)]\n",
    "            self.meta_optimizer.apply_gradients(zip(mean_gradients, self.model.trainable_variables))\n",
    "\n",
    "    def predict(self, x, k_shot_model=None):\n",
    "        if k_shot_model is None:\n",
    "            k_shot_model = self.model\n",
    "        return k_shot_model.predict(x)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "def evaluate_model(maml, test_data):\n",
    "    k_shot_model = maml.adapt(test_data)\n",
    "    predictions = maml.predict(test_data['X_val'], k_shot_model)\n",
    "    return predictions\n",
    "\n",
    "# Main code\n",
    "file_path = '../../../Dataset/ADBE_Stock.csv'  # Đường dẫn tới file CSV\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Thiết lập các tham số\n",
    "task_size = 50\n",
    "num_tasks = 10\n",
    "look_back = 1  # Số bước lùi\n",
    "input_shape = (look_back,)  # Đảm bảo hình dạng đúng\n",
    "meta_lr = 0.001\n",
    "task_lr = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Tạo các tasks\n",
    "tasks = create_tasks(data, task_size, num_tasks)\n",
    "\n",
    "# Chuẩn bị dữ liệu cho từng task\n",
    "prepared_tasks = prepare_task_data(tasks, look_back)\n",
    "\n",
    "# Xây dựng và huấn luyện mô hình MAML\n",
    "model = build_model(input_shape)  # Đảm bảo hình dạng đầu vào đúng\n",
    "maml = MAML(model, meta_lr, task_lr)\n",
    "maml.meta_train(prepared_tasks, num_epochs)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "test_task = prepared_tasks[0]  # Sử dụng một task từ tập dữ liệu đã chuẩn bị để kiểm tra\n",
    "predictions = evaluate_model(maml, test_task)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
